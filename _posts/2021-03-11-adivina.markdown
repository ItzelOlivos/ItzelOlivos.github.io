---
layout: post
title: Robots que juegan "Adivina Qui√©n"
date: 2021-03-11 00:00:00 +0300
description: spanish
img: adivina.jpg
tags: [deep learning] # add tag
---
Reinforcement Learning (RL) se ha convertido en la bala de plata detr√°s de muchos avances en rob√≥tica; hoy en d√≠a, por ejemplo, es com√∫n ver robots aut√≥nomos que aprenden, mediante RL, c√≥mo desarrollar tareas esenciales en los almacenes de Amazon (mira [esto](https://www.youtube.com/watch?v=PC-9HYJ1nCI)!). No obstante, los algoritmos de tipo RL requieren que un humano experto dise√±e una funci√≥n de recompensa capaz de codificar el objetivo de la tarea. ¬°Esto suena s√∫per raro! pero como los robots no hablan ni espa√±ol, ni ingl√©s, ni chino, ¬øde qu√© otra manera podr√≠amos hacerles saber qu√© es exactamente lo que queremos que hagan?

En la Universidad de Stanford, la Doctora Dorsa Sadigh tiene una idea mejor. En su art√≠culo de investigaci√≥n [‚ÄúLearning reward functions by integrating human demonstrations and preferences‚Äú](https://arxiv.org/abs/1906.08928), la Doctora muestra c√≥mo es que, mediante demostraciones de un humano experto y una serie de preguntas hechas por el robot, √©ste puede sintetizar por s√≠ solo la funci√≥n de recompensa que codifica la tarea que debe realizar.

La idea de permitir al robot aprender la funci√≥n de recompensa mediante demostraciones no es nueva, se conoce como Inverse Reinforcement Learning (IRL) desde hace varios a√±os; sin embargo, IRL requiere que un humano opere de manera remota un robot cuya fisionom√≠a es distinta a la nuestra (el robot Fetch que vemos en la imagen, por ejemplo, pareciera tener un brazo ¬°con tres codos! esto, naturalmente, causa demostraciones imprecisas). La idea innovadora de la Doctora Sadigh es complementar IRL con una t√©cnica de aprendizaje, conocida como Active Learning, que le permite al robot hacer preguntas sobre las preferencias del humano hasta encontrar la funci√≥n de recompensa que mejor codifica la tarea que √©ste debe realizar.

El proceso es simple: 1. El humano le demuestra al robot qu√© desea que √©ste haga al operarlo de manera remota y ejecutar la tarea de inter√©s (por ejemplo, recolectar un objeto y moverlo hasta una nueva posici√≥n). 2. El robot genera una distribuci√≥n de probabilidad sobre las trayectorias (o hip√≥tesis) que podr√≠an explicar lo que el humano intenta demostrar. 3. El robot elige el par de hip√≥tesis que maximizan la cantidad de informaci√≥n que se podr√≠a obtener tras recibir la respuesta del humano. 4. El humano elige cu√°l de las hip√≥tesis es la m√°s cercana a lo que √©l o ella intenta demostrar. 5. la distribuci√≥n de probabilidad sobre todas las posibles hip√≥tesis se actualiza. y 6. El proceso contin√∫a desde el paso 3 hasta que el robot tiene confianza en que cierta hip√≥tesis es la verdadera. B√°sicamente, el robot juega Adivina Qui√©n con el human, solo que en lugar de personajes, el robot propone trayectorias, y en lugar de preguntar si ‚Äúsu personaje tiene barba‚Äù, pregunta si hip√≥tesis X se acerca m√°s o menos que hip√≥tesis Y a su verdadera intenci√≥n üôÇ

En conclusi√≥n, este algoritmo permite que humanos no expertos en rob√≥tica ense√±en a las m√°quinas c√≥mo resolver tareas de una manera intuitiva.
