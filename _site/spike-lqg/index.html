<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Spike-Constrained Stochastic Control - Itzel Olivos-Castillo</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Itzel Olivos-Castillo" property="og:site_name">
  
    <meta content="Spike-Constrained Stochastic Control" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="projects" property="og:description">
  
  
    <meta content="http://localhost:4000/spike-lqg/" property="og:url">
  
  
    <meta content="2022-08-15T05:32:20-05:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/animation-spikes.gif" property="og:image">
  
  
    
  
  
    
    <meta content="control" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@_ItzelCoral">
  
    <meta name="twitter:title" content="Spike-Constrained Stochastic Control">
  
  
    <meta name="twitter:url" content="http://localhost:4000/spike-lqg/">
  
  
    <meta name="twitter:description" content="projects">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/animation-spikes.gif">
  

	<meta name="description" content="projects">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$$','$$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\(","\\)"] ],
        },
        TeX: {
          Macros: {
            bra: ["\\langle{#1}|", 1],
            ket: ["|{#1}\\rangle", 1],
            braket: ["\\langle{#1}\\rangle", 1],
            bk: ["\\langle{#1}|{#2}|{#3}\\rangle", 3]
         }
       }
      });
    </script>

    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML' async></script>
</head>

<body>

  <div class="wrapper">
    <script type="text/javascript">
  document.getElementById('cat_buttons').style.display = 'flex';

  function dispProjects() {
    document.getElementById('dispProj').style.display = 'flex';
    document.getElementById('dispPosts').style.display = 'none';
    document.getElementById('dispNews').style.display = 'none';
    document.getElementById('dispHobbies').style.display = 'none';

    document.getElementById('btnProjects').style.backgroundColor = 'black';
    document.getElementById('btnProjects').style.color = 'white';
    document.getElementById('btnPosts').style.backgroundColor = 'white'
    document.getElementById('btnPosts').style.color = 'black';
    document.getElementById('btnNews').style.backgroundColor = 'white';
    document.getElementById('btnNews').style.color = 'black';
    document.getElementById('btnHobbies').style.backgroundColor = 'white';
    document.getElementById('btnHobbies').style.color = 'black';
  }
  function dispPosts() {
    document.getElementById('dispProj').style.display = 'none';
    document.getElementById('dispPosts').style.display = 'flex';
    document.getElementById('dispNews').style.display = 'none';
    document.getElementById('dispHobbies').style.display = 'none';

    document.getElementById('btnProjects').style.backgroundColor = 'white';
    document.getElementById('btnProjects').style.color = 'black';
    document.getElementById('btnPosts').style.backgroundColor = 'black';
    document.getElementById('btnPosts').style.color = 'white';
    document.getElementById('btnNews').style.backgroundColor = 'white';
    document.getElementById('btnNews').style.color = 'black';
    document.getElementById('btnHobbies').style.backgroundColor = 'white';
    document.getElementById('btnHobbies').style.color = 'black';
  }
  function dispNews() {
    document.getElementById('dispProj').style.display = 'none';
    document.getElementById('dispPosts').style.display = 'none';
    document.getElementById('dispNews').style.display = 'flex';
    document.getElementById('dispHobbies').style.display = 'none';

    document.getElementById('btnProjects').style.backgroundColor = 'white';
    document.getElementById('btnProjects').style.color = 'black';
    document.getElementById('btnPosts').style.backgroundColor = 'white';
    document.getElementById('btnPosts').style.color = 'black';
    document.getElementById('btnNews').style.backgroundColor = 'black';
    document.getElementById('btnNews').style.color = 'white';
    document.getElementById('btnHobbies').style.backgroundColor = 'white';
    document.getElementById('btnHobbies').style.color = 'black';
  }
  function dispHobbies() {
    document.getElementById('dispProj').style.display = 'none';
    document.getElementById('dispPosts').style.display = 'none';
    document.getElementById('dispNews').style.display = 'none';
    document.getElementById('dispHobbies').style.display = 'flex';

    document.getElementById('btnProjects').style.backgroundColor = 'white';
    document.getElementById('btnProjects').style.color = 'black';
    document.getElementById('btnPosts').style.backgroundColor = 'white';
    document.getElementById('btnPosts').style.color = 'black';
    document.getElementById('btnNews').style.backgroundColor = 'white';
    document.getElementById('btnNews').style.color = 'black';
    document.getElementById('btnHobbies').style.backgroundColor = 'black';
    document.getElementById('btnHobbies').style.color = 'white';

  }
</script>

<aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/itzel.jpg" alt="Itzel Olivos-Castillo"></a>
      </div>
      <div class="author-name">Itzel Olivos-Castillo</div>
      <p><br> üíº PhD student at <a href="https://csweb.rice.edu">Rice University</a> (<a href="https://xaqlab.com">LAB-lab</a>) <br> üèÜ Fulbrighter 2019 - 2022 <br> üéì BSc and MSc degrees from <a href="https://www.youtube.com/watch?v=vjSRL3_3DGU&t=210s">IPN</a>, Mexico <br> ‚ù§Ô∏è Control, RL, Neuroscience</p>
    </div>
    <div id='cat_buttons' class="about">
      <button type="button" id="btnProjects" value="Projects" onclick="dispProjects()"
              style="background-color: black; border-color: black; color: white; border-width: 1px">Code</button>&nbsp;
      <button type="button" id="btnPosts" value="Posts" onclick="dispPosts()"
              style="background-color: white; border-color: black; color: black; border-width: 1px" >Posts</button>&nbsp;
      <button type="button" id="btnNews" value="News" onclick="dispNews()"
              style="background-color: white; border-color: black; color: black; border-width: 1px">News</button>&nbsp;
      <button type="button" id="btnHobbies" value="News" onclick="dispHobbies()"
              style="background-color: white; border-color: black; color: black; border-width: 1px">Fun</button>&nbsp;
    </div>

  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/_ItzelCoral" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/ItzelOlivos" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="email"><a href="mailto:itzel@rice.edu"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2022 &copy; Itzel Olivos-Castillo</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <script>
  document.getElementById('cat_buttons').style.display = 'none';
</script>

<article class="article-page">
  <div class="page-content">
    
      <div class="page-cover-image">
        
            <figure>
              <img class="page-image" src=/assets/img/animation-spikes.gif alt="Spike-Constrained Stochastic Control">
              
            </figure>
        
      </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Spike-Constrained Stochastic Control</h1>
        <div class="page-date"><span>2022, Aug 15&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p>what did, why matters, what was cool even if it didn‚Äôt work in the end</p>

<p>We develop a version of stochastic control that accounts for computational costs in the brain.  Motor control  and  reinforcement  learning  both  appeal  to  the  conceptual  framework  of  accumulating  evidence  about the  world  and  selecting  actions  based  on  the  synthesized  information  to  maximize  total  expected  utility. Yet neither of these approaches consider the costs of performing computations.  Conversely, past studies identified metabolically efficient ways of coding sensory information, but these studies are restricted to feedforward settings and static environments, and do not consider the consequences for closed-loop control. Here we combine concepts of efficient coding with control theory to analyze Linear Quadratic Gaussian (LQG) control, a well-understood mathematical example of optimal control that combines a Kalman filter for partially observed stochastic linear dynamics of a gaussian world state, with a linear regulator that minimizes integrated quadratic state costs and action costs.  We implement the Kalman filter neurally using a dynamic Probabilistic Population Code, in which linear projections of spiking neural activity approximate the natural parameters of a Gaussian posterior over theworld state. To this framework we add a cost on the total integrated number of spikes. In the simplest version, the precision of the inference is directly proportional to the number of spikes.  This creates a trade-off: an agent can obtain more utility overall by relinquishing some task performance, if doing so saves enough spikes.  By solving this problem, we describe how the optimal spike rate varies with the properties of the system to be controlled, such as stability, process noise, and observation noise. We discuss differences between how the brain should efficiently allocate resources in a closed-loop setting compared to the conventional feedforward setting.  Overall, this work provides  a  foundation  for  a  new  type  of  bounded  rational  behavior  that  could  be  used  to  explain  suboptimal computations in the brain</p>

<h2 id="methods">Methods</h2>
<p>Computing optimal control policies in general settings is intractable; an exception is the Linear Quadratic Gaussian case where the dynamics are linear, the noise is Gaussian, and the cost function is quadratic. Under these assumptions, an agent that computes control policies using Bellman‚Äôs optimality principle and maintains beliefs using a Kalman filter is optimal. Thus, to be concrete, we analyze an LQG control task and study the performance of an agent that aims to minimize the deviation of a controllable state $s$ from a target, while also minimizing the cost of its actions $u$. The dynamics of the state $s$ are governed by the stochastic differential equation $\dot{s}<em>t = As_t + Bu_t + \eta_t$, where $A$ is the state-transition matrix, $B$ is the control-input matrix, and $\eta_t$ is additive white Gaussian noise with isotropic covariance $\Sigma</em>\eta=\sigma_\eta^2\mathbb{I}$. The cost rate, $C‚Äô_{\rm{sys}}(s, u)$, penalizes the agent as it deviates from the target state or takes large actions; the magnitudes of these penalties are determined by time-varying weights $Q^{s}_t$ and $Q^{a}_t$, respectively. <br /><br /> The starting point to solve the control task in a biologically plausible manner is to encode the probability distribution over the observations using neural activity. We assume that the brain represents probabilities over the world state using a Probabilistic Population Code (PPCs), which is a distributed neural representation that support evidence integration through linear operations and marginalization through nonlinear operations \cite{beck2011marginalization}. Accordingly, spike trains, $\boldsymbol{r}_t^{\rm in}$, emitted by an input layer of neurons with Poisson-like response variability encode observations about the Gaussian world state $s_t$. Next, the responses $\boldsymbol{r}_t^{\rm in}$ feed a recurrent layer whose firing activity, $\boldsymbol{r}_t^{\rm out} \sim {\rm Poi}(\boldsymbol{v_t})$, encodes the belief that the agent would obtain by implementing recursive Bayesian estimation based on its last observation and action executed. Finally, the agent decodes the world state $s_t$ using different projections of the activity $\boldsymbol{r}_t^{\rm out}$, that is, $p(s_t|\boldsymbol{r}_t^{\rm out}) \propto \exp\big(-\frac{1}{2}s^2_t \boldsymbol{a} \cdot \boldsymbol{r}_t^{\rm out} + s_t \boldsymbol{b} \cdot \boldsymbol{r}_t^{\rm out}\big)$. The sufficient statistics for this posterior constitute the ``belief state‚Äô‚Äô $b_t$ of the system, such that $p(s_t|\boldsymbol{r}_t^{\rm out})=p(s_t|b_t)$.</p>

<blockquote>
  <p>Highlight note.</p>
</blockquote>

<p>The agent minimizes expected state and action cost by selecting actions $u_t = -L_t (\boldsymbol{b} \cdot \boldsymbol{r}<em>t^{\rm out}/\boldsymbol{a} \cdot \boldsymbol{r}_t^{\rm out})$, where $L_t$ is a control gain proportional to the solution of the Riccati equation $-\dot{\mathbb{S}} = Q_t^s + 2A\mathbb{S} - B^2\mathbb{S}^2/Q_t^u$. The minimal value of the integrated quadratic state and action costs, $C</em>{\rm sys}$, consists of irreducible terms that arise due to errors introduced by disturbances in the system and uncertainty in the initial state; hence, as in \cite{susemihl2014optimal}, our goal is to find neural representations that minimize the integrated estimation error $\int (B^2 \mathbb{S}_t^2 \sigma^2_t /Q^{u}_t) dt$ {\color{red}[Aren‚Äôt we trying to minimize TOTAL cost?]}. In contrast to \cite{susemihl2014optimal}, which optimizes the sensory apparatus, here we optimize the dynamic representation of the  belief state $b_t$ to minimize the task and representational costs.</p>

<ul>
  <li>Sample list</li>
  <li>Another item</li>
</ul>

<h2 id="results">Results</h2>
<p>For PPCs using a population of neurons with dense uniformly spaced tuning curves, the expected uncertainty in the observations is approximately independent of position, and inversely proportional to the sensory gain $g^{\rm in}$. The dynamic PPC encodes beliefs given a fixed sensory gain that yields time-dependent distributions whose natural parameter, $\boldsymbol{a}!\cdot!\boldsymbol{r}<em>t^{\rm out}$, converges quickly to an equilibrium determined by properties of the system to be controlled, including open-loop stability given by the transition matrix $A$, the process noise $\Sigma</em>\eta$, and the sensory gain $g^{\rm in}$, width $w$, and spacing $\Delta \theta$ of the tuning curves. Figure \ref{Fig:Data1} shows that the optimal network uses a gain smaller than 1 to reduce its spike count while stabilizing the system through closed-loop control. The optimal recurrent gain depends on the sensory gain, with stronger inputs permitting weaker inferences. %We show that the resultant cost rate to encode observations is given by $C‚Äô<em>{\rm obs} = (\sqrt{2\pi} g^{\rm in} w/ \Delta \theta) dt$, while the cost rate to encode beliefs is $C‚Äô</em>{\rm bel} = (N\alpha_t / \sigma^2_t) dt$, where $\alpha_t$ is a baseline parameter required to shift the neural responses to ensure positive firing rates as the state changes.</p>

<p>We also consider an active sensing version in which the input gain $g^{\rm in}$ can change over time. Finding sequences of $g_t^{\rm in}$ that minimize $C_{\rm sys}$ subject to a budget of spikes then becomes a trajectory optimization problem. We use Deep Temporal Difference Learning \cite{mnih2015human} to compute approximately optimal sensing policies; in this setting, the agent allocates more spikes in states where making mistakes is highly punished, a strategy that is often observed in behavioral experiments, and may provide a prediction for arousal signals reflected in neural activity and pupil dilation.</p>

<p>\({X}_{0}\) (works)
\(X_0\) (works)</p>

<p>Only in line equations work. Github does not support otherwise, we had to edit head.html to modify the script that converts math, seems difficult to handle inline math \(x_0\)</p>

      <div class="page-footer">
        <div class="page-share">
          <h4>Share it on:</h4>
          <a href="https://twitter.com/intent/tweet?text=Spike-Constrained Stochastic Control&url=http://localhost:4000/spike-lqg/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/spike-lqg/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#control" class="tag">&#35; control</a>
          
        </div>
      </div>
    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>
  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
